{"title":"Parallelization on Workbench","markdown":{"yaml":{"title":"Parallelization on Workbench","author":"Trishang Udhwani","format":{"html":{"toc":true,"toc-location":"left","toc-depth":2,"number-sections":true,"number-depth":3}},"editor":"visual"},"headingText":"What is Parallelization?","containsRefs":false,"markdown":"\n\n\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can have multiple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\n\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\n![](parallelization-on-workbench-images/download.png){fig-align=\"center\"}\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here is an example of four quad-core processors for a total of 16 cores in this machine.\n\n![](parallelization-on-workbench-images/download%20(1).png){fig-align=\"center\"}\n\nYou can think of this as allowing 16 computations to happen at the same time. Theoretically, your computation would take 1/16 of the time. Historically, R has only utilized one processor, which makes it single-threaded.\n\n## The lapply() function\n\nThe `lapply()` function has two arguments:\n\n1.  A list, or an object that can be coerced to a list.\n\n2.  A function to be applied to each element of the list\n\nThe `lapply()` function works much like a loop. It cycles through each element of the list and applies the supplied function to that element. While `lapply()` is applying your function to a list element, the other elements of the list are just…sitting around in memory. In the description of `lapply()`, there’s no mention of the different elements of the list communicating with each other, and the function being applied to a given list element does not need to know about other list elements.\n\nJust about any operation that is handled by the `lapply()` function can be parallelized. The idea is that a list object can be split across multiple cores of a processor and then the function can be applied to each subset of the list object on each of the cores. Conceptually, the steps in the parallel procedure are\n\n1.  Split list `X` across multiple cores\n\n2.  Copy the supplied function (and associated environment) to each of the cores\n\n3.  Apply the supplied function to each subset of the list `X` on each of the cores in parallel\n\n4.  Assemble the results of all the function evaluations into a single list and return\n\nThe differences between the many packages/functions in R essentially come down to how each of these steps are implemented.\n\n## The Parallel Package\n\nThe `parallel` package which comes with your R installation.\n\nThe `mclapply()` function essentially parallelizes calls to `lapply()`. The first two arguments to `mclapply()` are exactly the same as they are for `lapply()`. However, `mclapply()` has further arguments (that must be named), the most important of which is the `mc.cores` argument which you can use to specify the number of processors/cores you want to split the computation across. For example, if your machine has 4 cores on it, you might specify `mc.cores = 4` to break your parallelize your operation across 4 cores (although this may not be the best idea if you are running other operations in the background besides R).\n\nBriefly, your R session is the main process and when you call a function like `mclapply()`, you fork a series of sub-processes that operate independently from the main process (although they share a few low-level features). These sub-processes then execute your function on their subsets of the data, presumably on separate cores of your CPU. Once the computation is complete, each sub-process returns its results and then the sub-process is killed. The `parallel` package manages the logistics of forking the sub-processes and handling them once they’ve finished.\n\n::: callout-caution\nBecause of the use of the fork mechanism, the `mc*` functions are generally not available to users of the Windows operating system.\n:::\n\nThe first thing you might want to check with the `parallel` package is if your computer in fact has multiple cores that you can take advantage of.\n\n```{r}\nlibrary(parallel)\nparallel::detectCores()\n```\n\n::: callout-warning\nIn general, the information from `detectCores()` should be used cautiously as obtaining this kind of information from Unix-like operating systems is not always reliable. If you are going down this road, it’s best if you get to know your hardware better in order to have an understanding of how many CPUs/cores are available to you.\n:::\n\n### `mclapply()`\n\n```{r}\nset.seed(1)\n# Create a dataframe\ndf <- data.frame(replicate(1000, rnorm(10000)))\n\n# Using lapply() to find mean of each row\ns <- system.time({\n  list_means_1 <- lapply(1:nrow(df), function(i) mean(as.numeric(df[i, ])))\n})\nprint(s)\n```\n\nNote that in the `system.time()` output in first case, the `user` time and the `elapsed` time are roughly the same, which is what we would expect because there was no parallelization.\n\n```{r}\nlibrary(parallel)\n\n# Using mclapply() to find mean of each row\nnumberOfCores <- 4\ns <- system.time({\n  list_means_2 <- parallel::mclapply(1:nrow(df), function(i) mean(as.numeric(df[i, ])), mc.cores = numberOfCores)\n})\nprint(s)\n```\n\nYou’ll notice that the the `elapsed` time is now less than the `user` time. However, in general, the `elapsed` time will not be 1/4th of the `user` time, which is what we might expect with 4 cores if there were a perfect performance gain from parallelization.\n\nR keeps track of how much time is spent in the main process and how much is spent in any child processes.\n\n```{r}\ns[\"user.self\"]  # Main process\ns[\"user.child\"] # Child processes\n```\n\n::: callout-important\nOne advantage of serial computations is that it allows you to better keep a handle on how much **memory** your R job is using. When executing parallel jobs via `mclapply()` it’s important to pre-calculate how much memory *all* of the processes will require and make sure this is less than the total amount of memory on your computer.\n:::\n\nThe `mclapply()` function is useful for iterating over a single list or list-like object. If you have to iterate over multiple objects together, you can use `mcmapply()`, which is the the multi-core equivalent of the `mapply()` function.\n\n### Error Handling\n\nThis error handling behavior is a significant difference from the usual call to `lapply()`. With `lapply()`, if the supplied function fails on one component of the list, the entire function call to `lapply()` fails and you only get an error as a result.\n\nWith `mclapply()`, when a sub-process fails, the return value for that sub-process will be an R object that inherits from the class `\"try-error\"`, which is something you can test with the `inherits()` function. Conceptually, each child process is executed with the `try()` function wrapped around it. The code below deliberately causes an error in the 3 element of the list.\n\n```{r}\nr <- parallel::mclapply(1:5, function(i) {\n        if(i == 3L)\n                stop(\"error in this process!\")\n        else\n                return(\"success!\")\n}, mc.cores = 5)\n```\n\nHere we see there was a warning but no error in the running of the above code. We can check the return value.\n\n```{r}\nstr(r)\n```\n\nNote that the 3rd list element in `r` is different.\n\n```{r}\nclass(r[[3]])\ninherits(r[[3]], \"try-error\")\n```\n\nWhen running code where there may be errors in some of the sub-processes, it’s useful to check afterwards to see if there are any errors in the output received.\n\n```{r}\nbad <- sapply(r, inherits, what = \"try-error\")\nbad\n```\n\n### Generating Random Numbers\n\n```{r}\nset.seed(1)\nr <- parallel::mclapply(1:5, function(i) {\n        rnorm(3)\n}, mc.cores = 4)\n\nstr(r)\n```\n\nHowever, the above expression is not **reproducible** because the next time you run it, you will get a different set of random numbers. You cannot simply call `set.seed()` before running the expression as you might in a non-parallel version of the code.\n\nThe `parallel` package provides a way to reproducibly generate random numbers in a parallel environment via the “L’Ecuyer-CMRG” random number generator. Note that this is not the default random number generator so you will have to set it explicitly.\n\n```{r}\nRNGkind(\"L'Ecuyer-CMRG\")\nset.seed(1)\nr <- parallel::mclapply(1:5, function(i) {\n        rnorm(3)\n}, mc.cores = 4)\n\nstr(r)\n```\n\n`mclapply()` documentation can be found here: [mcapply()](https://www.rdocumentation.org/packages/parallel/versions/3.4.0/topics/mclapply)\n\n### The ParLapply() function\n\nUsing the forking mechanism on your computer is one way to execute parallel computation but it’s not the only way that the `parallel` package offers. Another way to build a “cluster” using the multiple cores on your computer is via *sockets*. A is simply a mechanism with which multiple processes or applications running on your computer (or different computers, for that matter) can communicate with each other. With parallel computation, data and results need to be passed back and forth between the parent and child processes and sockets can be used for that purpose.\n\n```{r}\nclu <- parallel::makeCluster(4)\n```\n\nThe `clu` object is an abstraction of the entire cluster and is what we’ll use to indicate to the various cluster functions that we want to do parallel computation.\n\nTo do an `lapply()` operation over a socket cluster we can use the `parLapply()` function.\n\n```{r error=TRUE}\nlist_means_3 <- parallel::parLapply(clu, 1:nrow(df), function(i) mean(as.numeric(df[i, ]))) \n```\n\nUnfortunately, that there’s an error in running this code. The reason is that while we have loaded the df data into our R session, the data is not available to the independent child processes that have been spawned by the `makeCluster()` function. The *socket* approach launches a new version of R on each core whereas the *forking* approach copies the entire current version of R and moves it to a new core.\n\nThe data, and any other information that the child process will need to execute your code, needs to be **exported** to the child process from the parent process via the `clusterExport()` function. The need to export data is a key difference in behavior between the “multicore” approach and the “socket” approach.\n\n```{r}\nparallel::clusterExport(clu, \"df\")\n```\n\nThe second argument to `clusterExport()` is a character vector, and so you can export an arbitrary number of R objects to the child processes. You should be judicious in choosing what you export simply because each R object will be replicated in each of the child processes, and hence take up memory on your computer.\n\n```{r}\nlist_means_3 <- parallel::parLapply(clu, 1:nrow(df), function(i) mean(as.numeric(df[i, ]))) \n```\n\nOnce you’ve finished working with your cluster, it’s good to clean up and stop the cluster child processes (quitting R will also stop all of the child processes).\n\n```{r}\nparallel::stopCluster(clu)\n```\n\n::: callout-note\nSometimes we will also need to load the packages in individual child processes. This can be done by using `clusterEvalQ` . For example:\n\n``` r\nparallel::clusterEvalQ(clu, {\n  library(ggplot2)\n  library(stringr)\n})\n```\n:::\n\n`ParLapply()` is a part of `clusterApply()` family of functions. The documentation can be found here: [clusterApply()](https://www.rdocumentation.org/packages/parallel/versions/3.6.2/topics/clusterApply)\n\n## `foreach` and `doParallel` Package\n\nThe normal `for` loop in R looks like:\n\n```{r}\nfor (i in 1:3) {\n  print(sqrt(i))\n}\n```\n\nThe `foreach` method is similar, but uses the sequential `%do%` operator to indicate an expression to run. Note the difference in the returned data structure.\n\n```{r}\nlibrary(foreach)\nforeach (i=1:3) %do% {\n  sqrt(i)\n}\n```\n\n### `%dopar%` operator\n\nIn addition, `foreach` supports a parallelizable operator `%dopar%` from the `doParallel` package. This allows each iteration through the loop to use different cores or different machines in a cluster.\n\n```{r}\nlibrary(foreach)\nlibrary(doParallel)\n\ndoParallel::registerDoParallel(4) \nforeach (i=1:5) %dopar% {\n  sqrt(i)\n}\n```\n\nTo simplify output, `foreach` has the `.combine` parameter that can simplify return values\n\n```{r}\nforeach (i=1:3, .combine=c) %dopar% {\n  sqrt(i)\n}\n```\n\n`foreach` also has the `.rbind` parameter that can return a dataframe\n\n```{r}\nforeach (i=1:3, .combine=rbind) %dopar% {\n  sqrt(i)\n}\n```\n\nThe [doParallel vignette](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf) on CRAN shows a much more realistic example, where one can use `%dopar%` to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined. Here use the iris data set to do a parallel bootstrap:\n\n```{r}\n\nx <- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials <- 10000\nsystem.time({\n  r <- foreach(icount(trials), .combine=rbind) %dopar% {\n    ind <- sample(100, 100, replace=TRUE)\n    result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n```\n\nAnd compare that to what it takes to do the same analysis in serial:\n\n```{r}\nsystem.time({\n  r <- foreach(icount(trials), .combine=rbind) %do% {\n    ind <- sample(100, 100, replace=TRUE)\n    result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n```\n\nWhen we're done, we will clean up the cluster:\n\n```{r}\ndoParallel::stopImplicitCluster()\n```\n\n### `%dorng%` operator\n\nstandard `%dopar%` loops are not reproducible:\n\nFirst, let's set the RNGkind back to default\n\n```{r}\nRNGkind(\"default\")\n```\n\nNow register a new cluster\n\n```{r}\ndoParallel::registerDoParallel(4)\n```\n\n```{r}\nset.seed(123)\nres <- foreach(i=1:5) %dopar% { runif(3) }\nset.seed(123)\nres2 <- foreach(i=1:5) %dopar% { runif(3) }\nidentical(res, res2)\n```\n\nThe doRNG package provides convenient ways to implement reproducible parallel `foreach` loops, independently of the parallel backend used to perform the computation.\n\n```{r}\nlibrary(doRNG)\nset.seed(123)\nres <- foreach(i=1:5) %dorng% { runif(3) }\nset.seed(123)\nres2 <- foreach(i=1:5) %dorng% { runif(3) }\nidentical(res, res2)\n```\n\nWhen we're done, we will clean up the cluster:\n\n```{r}\ndoParallel::stopImplicitCluster()\n```\n\nThe `doParallel` documentation can be found here: [doParallel](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf)\n\n## The future Package\n\nThe `future` package defines *plans* to specify how computations are executed. A plan can use multiple cores, separate R sessions, or even remote systems.\n\n```{r}\nlibrary(future)\nplan(multisession, workers = 4)\n\n# Define a future\nf <- future::future({ sum(1:1e6) })\n# Retrieve the result\nresult <- future::value(f)\nprint(result)\n```\n\n::: callout-note\nWe could have used `plan(multicore)` instead of `plan(multisession)` if we were working directly in R and not RStudio. However, `plan(multicore)` will only work in a Linux/macOS environment\n:::\n\nThe `future` package is a lot more comprehensive but beyond the scope of discussion for this basic tutorial. If you are interested, the documentation can be found here: [future package](https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html)\n\nThis is how we can use it instead of `lapply`\n\n```{r}\nlibrary(future.apply)\nplan(multisession, workers = 4)\nresult <- future.apply::future_lapply(1:5, function(x) x^2)\nresult\n```\n\n`Multisession` runs background R sessions on the current machine. For large parallelizations, we should run `future_lapply` by defining a cluster manually. That way we can run it on external R sessions on current, local, and/or remote machines.\n\n```{r}\nlibrary(parallel)\nclu <- parallel::makeCluster(4)\nplan(cluster, workers = clu)\nresult <- future_lapply(1:10, function(x) x^2)\nstopCluster(clu)\n```\n\nDocumentation for `future.apply` family of functions can be found here: [future.apply documentation](https://cran.r-project.org/web/packages/future.apply/vignettes/future.apply-1-overview.html)\n\n### Integration with SLURM\n\nFuture works very well with SLURM integration. The `future.batchtools` package extends the `future` ecosystem for SLURM. Here is an example code:\n\n``` r\nlibrary(future.batchtools)\nplan(batchtools_slurm, template = \"sumbit_job.slurm\")\n\nf <- future::future({ Sys.sleep(10); sum(1:1e6) })\nresult <- future::value(f)\n```\n\nYou need a SLURM template file (`submit_job.slurm`) that specifies job parameters (e.g., cores, memory).\n\n## Other ways to parallelize R code\n\nThere are several other ways to parallelize your code. If you are looking for more packages, some of those are mentioned here along with their documentation. Some of these are extensions of packages already mentioned while others introduce different ways to parallelize.\n\n-   [`furrr` package](https://furrr.futureverse.org/)\n\n-   [`doFuture` package](https://cran.r-project.org/web/packages/doFuture/doFuture.pdf)\n\n-   [`RcppParallel` package](https://rcppcore.github.io/RcppParallel/)\n\n-   [`parallelMap` package](https://cran.r-project.org/web/packages/parallelMap/parallelMap.pdf)\n\n-   [`batchtools` package](https://github.com/mllg/batchtools)\n\n## Saving results while parallelizing\n\nWhen running large computations, it may be helpful to save results iteratively or as checkpoints to avoid data loss in case of interruptions. Here is an example of saving results iteratively using the `foreach` and `doParallel` libraries\n\n```{r}\nlibrary(foreach)\nlibrary(doParallel)\n\n# Register parallel backend\ndoParallel::registerDoParallel(4)\n\n# Parallel computation and saving results\nresults <- foreach(i = 1:100, .combine = c) %dopar% {\n  # Your computation\n  Sys.sleep(0.1) # Simulates time-consuming computation\n  result <- i^2\n  \n  # Save intermediate results\n  saveRDS(result, file = paste0(\"for_each/result_\", i, \".rds\"))\n  result\n}\n\ndoParallel::stopImplicitCluster()\n```\n\nHere is another example using `future.apply` library\n\n```{r}\nlibrary(future.apply)\n\nplan(multisession, workers = 4)\n\n# Function with intermediate saving\nsafe_compute <- function(i) {\n  result <- i^2\n  saveRDS(result, file = paste0(\"future_apply/partial_result_\", i, \".rds\"))\n  return(result)\n}\n\n# Run computation\nresults <- future.apply::future_lapply(1:100, safe_compute)\n\n# Aggregate saved results\nfinal_results <- unlist(results)\nsaveRDS(final_results, file = \"future_apply/final_results.rds\")\n\n```\n\n### Designing a function to restore progress:\n\nHere is the \"structure\" of a function that can be used to restore your progress.\n\n```{r}\nlibrary(future.apply)\n\n# Define checkpoint directory\ncheckpoint_dir <- \"checkpoints_parallel\"\ndir.create(checkpoint_dir, showWarnings = FALSE)\n\n# Set up parallel plan\nplan(multisession, workers = 4)\n\n# Function to perform computations with checkpoints\ncompute_task <- function(i) {\n  checkpoint_file <- file.path(checkpoint_dir, paste0(\"result_\", i, \".rds\"))\n  \n  if (file.exists(checkpoint_file)) {\n    # Restore from checkpoint\n    result <- readRDS(checkpoint_file)\n  } else {\n    # Perform the computation\n    Sys.sleep(1)  # Simulates a time-consuming task\n    result <- i*2\n    \n    # Save checkpoint\n    saveRDS(result, checkpoint_file)\n  }\n  return(result)\n}\n\n# Parallel computation with checkpoints\nresults <- future.apply::future_lapply(1:10, compute_task)\n\n# Aggregate results\nfinal_results <- unlist(results)\nprint(final_results)\n```\n\n### Targets package for efficient checkpoint management\n\nThis package is a pipeline tool for statistics and data science in R. The package skips costly runtime for tasks that are already up to date, orchestrates the necessary computation with implicit parallel computing, and abstracts files as R objects. If all the current output matches the current upstream code and data, then the whole pipeline is up to date, and the results are more trustworthy than otherwise.\n\n```{r}\nlibrary(targets)\ntar_script({\n  library(future.apply)\n\n  # Parallelization plan\n  plan(multisession)\n\n  # Define the computation function with checkpointing\n  compute_with_checkpoint <- function(x, checkpoint_dir) {\n    checkpoint_file <- file.path(checkpoint_dir, paste0(\"result_\", x, \".rds\"))\n    if (file.exists(checkpoint_file)) {\n      result <- readRDS(checkpoint_file)\n    } else {\n      Sys.sleep(2)  # Simulate a long computation\n      result <- x^2\n      saveRDS(result, checkpoint_file)\n    }\n    return(result)\n  }\n\n  # Define the pipeline\n  tar_option_set(\n    packages = c(\"future.apply\"),\n    format = \"rds\"\n  )\n\n  list(\n    tar_target(\n      checkpoint_dir,\n      {\n        dir <- \"checkpoints_targets\"\n        dir.create(dir, showWarnings = FALSE)\n        dir\n      },\n      format = \"file\"\n    ),\n    tar_target(\n      data,\n      seq(1, 10),\n      format = \"rds\"\n    ),\n    tar_target(\n      results,\n      future_lapply(data, compute_with_checkpoint, checkpoint_dir = checkpoint_dir),\n      format = \"rds\"\n    ),\n    tar_target(\n      final_save,\n      {\n        saveRDS(results, \"final_results.rds\")\n        results\n      },\n      format = \"rds\"\n    )\n  )\n})\n```\n\n```{r}\ntar_make()\n\ntar_visnetwork()\n```\n\nThe documentation for `targets` package can be found here: [targets package documentation](https://books.ropensci.org/targets/)\n","srcMarkdownNoYaml":"\n\n## What is Parallelization?\n\nA modern CPU (Central Processing Unit) is at the heart of every computer. While traditional computers had a single CPU, modern computers can have multiple processors, which in turn can each contain multiple cores. These processors and cores are available to perform computations.\n\nA computer with one processor may still have 4 cores (quad-core), allowing 4 computations to be executed at the same time.\n\n![](parallelization-on-workbench-images/download.png){fig-align=\"center\"}\n\nA typical modern computer has multiple cores, ranging from one or two in laptops to thousands in high performance compute clusters. Here is an example of four quad-core processors for a total of 16 cores in this machine.\n\n![](parallelization-on-workbench-images/download%20(1).png){fig-align=\"center\"}\n\nYou can think of this as allowing 16 computations to happen at the same time. Theoretically, your computation would take 1/16 of the time. Historically, R has only utilized one processor, which makes it single-threaded.\n\n## The lapply() function\n\nThe `lapply()` function has two arguments:\n\n1.  A list, or an object that can be coerced to a list.\n\n2.  A function to be applied to each element of the list\n\nThe `lapply()` function works much like a loop. It cycles through each element of the list and applies the supplied function to that element. While `lapply()` is applying your function to a list element, the other elements of the list are just…sitting around in memory. In the description of `lapply()`, there’s no mention of the different elements of the list communicating with each other, and the function being applied to a given list element does not need to know about other list elements.\n\nJust about any operation that is handled by the `lapply()` function can be parallelized. The idea is that a list object can be split across multiple cores of a processor and then the function can be applied to each subset of the list object on each of the cores. Conceptually, the steps in the parallel procedure are\n\n1.  Split list `X` across multiple cores\n\n2.  Copy the supplied function (and associated environment) to each of the cores\n\n3.  Apply the supplied function to each subset of the list `X` on each of the cores in parallel\n\n4.  Assemble the results of all the function evaluations into a single list and return\n\nThe differences between the many packages/functions in R essentially come down to how each of these steps are implemented.\n\n## The Parallel Package\n\nThe `parallel` package which comes with your R installation.\n\nThe `mclapply()` function essentially parallelizes calls to `lapply()`. The first two arguments to `mclapply()` are exactly the same as they are for `lapply()`. However, `mclapply()` has further arguments (that must be named), the most important of which is the `mc.cores` argument which you can use to specify the number of processors/cores you want to split the computation across. For example, if your machine has 4 cores on it, you might specify `mc.cores = 4` to break your parallelize your operation across 4 cores (although this may not be the best idea if you are running other operations in the background besides R).\n\nBriefly, your R session is the main process and when you call a function like `mclapply()`, you fork a series of sub-processes that operate independently from the main process (although they share a few low-level features). These sub-processes then execute your function on their subsets of the data, presumably on separate cores of your CPU. Once the computation is complete, each sub-process returns its results and then the sub-process is killed. The `parallel` package manages the logistics of forking the sub-processes and handling them once they’ve finished.\n\n::: callout-caution\nBecause of the use of the fork mechanism, the `mc*` functions are generally not available to users of the Windows operating system.\n:::\n\nThe first thing you might want to check with the `parallel` package is if your computer in fact has multiple cores that you can take advantage of.\n\n```{r}\nlibrary(parallel)\nparallel::detectCores()\n```\n\n::: callout-warning\nIn general, the information from `detectCores()` should be used cautiously as obtaining this kind of information from Unix-like operating systems is not always reliable. If you are going down this road, it’s best if you get to know your hardware better in order to have an understanding of how many CPUs/cores are available to you.\n:::\n\n### `mclapply()`\n\n```{r}\nset.seed(1)\n# Create a dataframe\ndf <- data.frame(replicate(1000, rnorm(10000)))\n\n# Using lapply() to find mean of each row\ns <- system.time({\n  list_means_1 <- lapply(1:nrow(df), function(i) mean(as.numeric(df[i, ])))\n})\nprint(s)\n```\n\nNote that in the `system.time()` output in first case, the `user` time and the `elapsed` time are roughly the same, which is what we would expect because there was no parallelization.\n\n```{r}\nlibrary(parallel)\n\n# Using mclapply() to find mean of each row\nnumberOfCores <- 4\ns <- system.time({\n  list_means_2 <- parallel::mclapply(1:nrow(df), function(i) mean(as.numeric(df[i, ])), mc.cores = numberOfCores)\n})\nprint(s)\n```\n\nYou’ll notice that the the `elapsed` time is now less than the `user` time. However, in general, the `elapsed` time will not be 1/4th of the `user` time, which is what we might expect with 4 cores if there were a perfect performance gain from parallelization.\n\nR keeps track of how much time is spent in the main process and how much is spent in any child processes.\n\n```{r}\ns[\"user.self\"]  # Main process\ns[\"user.child\"] # Child processes\n```\n\n::: callout-important\nOne advantage of serial computations is that it allows you to better keep a handle on how much **memory** your R job is using. When executing parallel jobs via `mclapply()` it’s important to pre-calculate how much memory *all* of the processes will require and make sure this is less than the total amount of memory on your computer.\n:::\n\nThe `mclapply()` function is useful for iterating over a single list or list-like object. If you have to iterate over multiple objects together, you can use `mcmapply()`, which is the the multi-core equivalent of the `mapply()` function.\n\n### Error Handling\n\nThis error handling behavior is a significant difference from the usual call to `lapply()`. With `lapply()`, if the supplied function fails on one component of the list, the entire function call to `lapply()` fails and you only get an error as a result.\n\nWith `mclapply()`, when a sub-process fails, the return value for that sub-process will be an R object that inherits from the class `\"try-error\"`, which is something you can test with the `inherits()` function. Conceptually, each child process is executed with the `try()` function wrapped around it. The code below deliberately causes an error in the 3 element of the list.\n\n```{r}\nr <- parallel::mclapply(1:5, function(i) {\n        if(i == 3L)\n                stop(\"error in this process!\")\n        else\n                return(\"success!\")\n}, mc.cores = 5)\n```\n\nHere we see there was a warning but no error in the running of the above code. We can check the return value.\n\n```{r}\nstr(r)\n```\n\nNote that the 3rd list element in `r` is different.\n\n```{r}\nclass(r[[3]])\ninherits(r[[3]], \"try-error\")\n```\n\nWhen running code where there may be errors in some of the sub-processes, it’s useful to check afterwards to see if there are any errors in the output received.\n\n```{r}\nbad <- sapply(r, inherits, what = \"try-error\")\nbad\n```\n\n### Generating Random Numbers\n\n```{r}\nset.seed(1)\nr <- parallel::mclapply(1:5, function(i) {\n        rnorm(3)\n}, mc.cores = 4)\n\nstr(r)\n```\n\nHowever, the above expression is not **reproducible** because the next time you run it, you will get a different set of random numbers. You cannot simply call `set.seed()` before running the expression as you might in a non-parallel version of the code.\n\nThe `parallel` package provides a way to reproducibly generate random numbers in a parallel environment via the “L’Ecuyer-CMRG” random number generator. Note that this is not the default random number generator so you will have to set it explicitly.\n\n```{r}\nRNGkind(\"L'Ecuyer-CMRG\")\nset.seed(1)\nr <- parallel::mclapply(1:5, function(i) {\n        rnorm(3)\n}, mc.cores = 4)\n\nstr(r)\n```\n\n`mclapply()` documentation can be found here: [mcapply()](https://www.rdocumentation.org/packages/parallel/versions/3.4.0/topics/mclapply)\n\n### The ParLapply() function\n\nUsing the forking mechanism on your computer is one way to execute parallel computation but it’s not the only way that the `parallel` package offers. Another way to build a “cluster” using the multiple cores on your computer is via *sockets*. A is simply a mechanism with which multiple processes or applications running on your computer (or different computers, for that matter) can communicate with each other. With parallel computation, data and results need to be passed back and forth between the parent and child processes and sockets can be used for that purpose.\n\n```{r}\nclu <- parallel::makeCluster(4)\n```\n\nThe `clu` object is an abstraction of the entire cluster and is what we’ll use to indicate to the various cluster functions that we want to do parallel computation.\n\nTo do an `lapply()` operation over a socket cluster we can use the `parLapply()` function.\n\n```{r error=TRUE}\nlist_means_3 <- parallel::parLapply(clu, 1:nrow(df), function(i) mean(as.numeric(df[i, ]))) \n```\n\nUnfortunately, that there’s an error in running this code. The reason is that while we have loaded the df data into our R session, the data is not available to the independent child processes that have been spawned by the `makeCluster()` function. The *socket* approach launches a new version of R on each core whereas the *forking* approach copies the entire current version of R and moves it to a new core.\n\nThe data, and any other information that the child process will need to execute your code, needs to be **exported** to the child process from the parent process via the `clusterExport()` function. The need to export data is a key difference in behavior between the “multicore” approach and the “socket” approach.\n\n```{r}\nparallel::clusterExport(clu, \"df\")\n```\n\nThe second argument to `clusterExport()` is a character vector, and so you can export an arbitrary number of R objects to the child processes. You should be judicious in choosing what you export simply because each R object will be replicated in each of the child processes, and hence take up memory on your computer.\n\n```{r}\nlist_means_3 <- parallel::parLapply(clu, 1:nrow(df), function(i) mean(as.numeric(df[i, ]))) \n```\n\nOnce you’ve finished working with your cluster, it’s good to clean up and stop the cluster child processes (quitting R will also stop all of the child processes).\n\n```{r}\nparallel::stopCluster(clu)\n```\n\n::: callout-note\nSometimes we will also need to load the packages in individual child processes. This can be done by using `clusterEvalQ` . For example:\n\n``` r\nparallel::clusterEvalQ(clu, {\n  library(ggplot2)\n  library(stringr)\n})\n```\n:::\n\n`ParLapply()` is a part of `clusterApply()` family of functions. The documentation can be found here: [clusterApply()](https://www.rdocumentation.org/packages/parallel/versions/3.6.2/topics/clusterApply)\n\n## `foreach` and `doParallel` Package\n\nThe normal `for` loop in R looks like:\n\n```{r}\nfor (i in 1:3) {\n  print(sqrt(i))\n}\n```\n\nThe `foreach` method is similar, but uses the sequential `%do%` operator to indicate an expression to run. Note the difference in the returned data structure.\n\n```{r}\nlibrary(foreach)\nforeach (i=1:3) %do% {\n  sqrt(i)\n}\n```\n\n### `%dopar%` operator\n\nIn addition, `foreach` supports a parallelizable operator `%dopar%` from the `doParallel` package. This allows each iteration through the loop to use different cores or different machines in a cluster.\n\n```{r}\nlibrary(foreach)\nlibrary(doParallel)\n\ndoParallel::registerDoParallel(4) \nforeach (i=1:5) %dopar% {\n  sqrt(i)\n}\n```\n\nTo simplify output, `foreach` has the `.combine` parameter that can simplify return values\n\n```{r}\nforeach (i=1:3, .combine=c) %dopar% {\n  sqrt(i)\n}\n```\n\n`foreach` also has the `.rbind` parameter that can return a dataframe\n\n```{r}\nforeach (i=1:3, .combine=rbind) %dopar% {\n  sqrt(i)\n}\n```\n\nThe [doParallel vignette](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf) on CRAN shows a much more realistic example, where one can use `%dopar%` to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined. Here use the iris data set to do a parallel bootstrap:\n\n```{r}\n\nx <- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials <- 10000\nsystem.time({\n  r <- foreach(icount(trials), .combine=rbind) %dopar% {\n    ind <- sample(100, 100, replace=TRUE)\n    result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n```\n\nAnd compare that to what it takes to do the same analysis in serial:\n\n```{r}\nsystem.time({\n  r <- foreach(icount(trials), .combine=rbind) %do% {\n    ind <- sample(100, 100, replace=TRUE)\n    result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})\n```\n\nWhen we're done, we will clean up the cluster:\n\n```{r}\ndoParallel::stopImplicitCluster()\n```\n\n### `%dorng%` operator\n\nstandard `%dopar%` loops are not reproducible:\n\nFirst, let's set the RNGkind back to default\n\n```{r}\nRNGkind(\"default\")\n```\n\nNow register a new cluster\n\n```{r}\ndoParallel::registerDoParallel(4)\n```\n\n```{r}\nset.seed(123)\nres <- foreach(i=1:5) %dopar% { runif(3) }\nset.seed(123)\nres2 <- foreach(i=1:5) %dopar% { runif(3) }\nidentical(res, res2)\n```\n\nThe doRNG package provides convenient ways to implement reproducible parallel `foreach` loops, independently of the parallel backend used to perform the computation.\n\n```{r}\nlibrary(doRNG)\nset.seed(123)\nres <- foreach(i=1:5) %dorng% { runif(3) }\nset.seed(123)\nres2 <- foreach(i=1:5) %dorng% { runif(3) }\nidentical(res, res2)\n```\n\nWhen we're done, we will clean up the cluster:\n\n```{r}\ndoParallel::stopImplicitCluster()\n```\n\nThe `doParallel` documentation can be found here: [doParallel](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf)\n\n## The future Package\n\nThe `future` package defines *plans* to specify how computations are executed. A plan can use multiple cores, separate R sessions, or even remote systems.\n\n```{r}\nlibrary(future)\nplan(multisession, workers = 4)\n\n# Define a future\nf <- future::future({ sum(1:1e6) })\n# Retrieve the result\nresult <- future::value(f)\nprint(result)\n```\n\n::: callout-note\nWe could have used `plan(multicore)` instead of `plan(multisession)` if we were working directly in R and not RStudio. However, `plan(multicore)` will only work in a Linux/macOS environment\n:::\n\nThe `future` package is a lot more comprehensive but beyond the scope of discussion for this basic tutorial. If you are interested, the documentation can be found here: [future package](https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html)\n\nThis is how we can use it instead of `lapply`\n\n```{r}\nlibrary(future.apply)\nplan(multisession, workers = 4)\nresult <- future.apply::future_lapply(1:5, function(x) x^2)\nresult\n```\n\n`Multisession` runs background R sessions on the current machine. For large parallelizations, we should run `future_lapply` by defining a cluster manually. That way we can run it on external R sessions on current, local, and/or remote machines.\n\n```{r}\nlibrary(parallel)\nclu <- parallel::makeCluster(4)\nplan(cluster, workers = clu)\nresult <- future_lapply(1:10, function(x) x^2)\nstopCluster(clu)\n```\n\nDocumentation for `future.apply` family of functions can be found here: [future.apply documentation](https://cran.r-project.org/web/packages/future.apply/vignettes/future.apply-1-overview.html)\n\n### Integration with SLURM\n\nFuture works very well with SLURM integration. The `future.batchtools` package extends the `future` ecosystem for SLURM. Here is an example code:\n\n``` r\nlibrary(future.batchtools)\nplan(batchtools_slurm, template = \"sumbit_job.slurm\")\n\nf <- future::future({ Sys.sleep(10); sum(1:1e6) })\nresult <- future::value(f)\n```\n\nYou need a SLURM template file (`submit_job.slurm`) that specifies job parameters (e.g., cores, memory).\n\n## Other ways to parallelize R code\n\nThere are several other ways to parallelize your code. If you are looking for more packages, some of those are mentioned here along with their documentation. Some of these are extensions of packages already mentioned while others introduce different ways to parallelize.\n\n-   [`furrr` package](https://furrr.futureverse.org/)\n\n-   [`doFuture` package](https://cran.r-project.org/web/packages/doFuture/doFuture.pdf)\n\n-   [`RcppParallel` package](https://rcppcore.github.io/RcppParallel/)\n\n-   [`parallelMap` package](https://cran.r-project.org/web/packages/parallelMap/parallelMap.pdf)\n\n-   [`batchtools` package](https://github.com/mllg/batchtools)\n\n## Saving results while parallelizing\n\nWhen running large computations, it may be helpful to save results iteratively or as checkpoints to avoid data loss in case of interruptions. Here is an example of saving results iteratively using the `foreach` and `doParallel` libraries\n\n```{r}\nlibrary(foreach)\nlibrary(doParallel)\n\n# Register parallel backend\ndoParallel::registerDoParallel(4)\n\n# Parallel computation and saving results\nresults <- foreach(i = 1:100, .combine = c) %dopar% {\n  # Your computation\n  Sys.sleep(0.1) # Simulates time-consuming computation\n  result <- i^2\n  \n  # Save intermediate results\n  saveRDS(result, file = paste0(\"for_each/result_\", i, \".rds\"))\n  result\n}\n\ndoParallel::stopImplicitCluster()\n```\n\nHere is another example using `future.apply` library\n\n```{r}\nlibrary(future.apply)\n\nplan(multisession, workers = 4)\n\n# Function with intermediate saving\nsafe_compute <- function(i) {\n  result <- i^2\n  saveRDS(result, file = paste0(\"future_apply/partial_result_\", i, \".rds\"))\n  return(result)\n}\n\n# Run computation\nresults <- future.apply::future_lapply(1:100, safe_compute)\n\n# Aggregate saved results\nfinal_results <- unlist(results)\nsaveRDS(final_results, file = \"future_apply/final_results.rds\")\n\n```\n\n### Designing a function to restore progress:\n\nHere is the \"structure\" of a function that can be used to restore your progress.\n\n```{r}\nlibrary(future.apply)\n\n# Define checkpoint directory\ncheckpoint_dir <- \"checkpoints_parallel\"\ndir.create(checkpoint_dir, showWarnings = FALSE)\n\n# Set up parallel plan\nplan(multisession, workers = 4)\n\n# Function to perform computations with checkpoints\ncompute_task <- function(i) {\n  checkpoint_file <- file.path(checkpoint_dir, paste0(\"result_\", i, \".rds\"))\n  \n  if (file.exists(checkpoint_file)) {\n    # Restore from checkpoint\n    result <- readRDS(checkpoint_file)\n  } else {\n    # Perform the computation\n    Sys.sleep(1)  # Simulates a time-consuming task\n    result <- i*2\n    \n    # Save checkpoint\n    saveRDS(result, checkpoint_file)\n  }\n  return(result)\n}\n\n# Parallel computation with checkpoints\nresults <- future.apply::future_lapply(1:10, compute_task)\n\n# Aggregate results\nfinal_results <- unlist(results)\nprint(final_results)\n```\n\n### Targets package for efficient checkpoint management\n\nThis package is a pipeline tool for statistics and data science in R. The package skips costly runtime for tasks that are already up to date, orchestrates the necessary computation with implicit parallel computing, and abstracts files as R objects. If all the current output matches the current upstream code and data, then the whole pipeline is up to date, and the results are more trustworthy than otherwise.\n\n```{r}\nlibrary(targets)\ntar_script({\n  library(future.apply)\n\n  # Parallelization plan\n  plan(multisession)\n\n  # Define the computation function with checkpointing\n  compute_with_checkpoint <- function(x, checkpoint_dir) {\n    checkpoint_file <- file.path(checkpoint_dir, paste0(\"result_\", x, \".rds\"))\n    if (file.exists(checkpoint_file)) {\n      result <- readRDS(checkpoint_file)\n    } else {\n      Sys.sleep(2)  # Simulate a long computation\n      result <- x^2\n      saveRDS(result, checkpoint_file)\n    }\n    return(result)\n  }\n\n  # Define the pipeline\n  tar_option_set(\n    packages = c(\"future.apply\"),\n    format = \"rds\"\n  )\n\n  list(\n    tar_target(\n      checkpoint_dir,\n      {\n        dir <- \"checkpoints_targets\"\n        dir.create(dir, showWarnings = FALSE)\n        dir\n      },\n      format = \"file\"\n    ),\n    tar_target(\n      data,\n      seq(1, 10),\n      format = \"rds\"\n    ),\n    tar_target(\n      results,\n      future_lapply(data, compute_with_checkpoint, checkpoint_dir = checkpoint_dir),\n      format = \"rds\"\n    ),\n    tar_target(\n      final_save,\n      {\n        saveRDS(results, \"final_results.rds\")\n        results\n      },\n      format = \"rds\"\n    )\n  )\n})\n```\n\n```{r}\ntar_make()\n\ntar_visnetwork()\n```\n\nThe documentation for `targets` package can be found here: [targets package documentation](https://books.ropensci.org/targets/)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"number-sections":true,"output-file":"parallelization on workbench.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","theme":"cosmo","title":"Parallelization on Workbench","author":"Trishang Udhwani","toc-location":"left","number-depth":3},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"parallelization on workbench.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"editor":"visual","documentclass":"scrreprt","title":"Parallelization on Workbench","author":"Trishang Udhwani"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}